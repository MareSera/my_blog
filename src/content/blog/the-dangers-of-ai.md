---
title: 'AI可能极其危险->无论它是否有意识'
description: '人工智能'
pubDate: 'Aug 01 2024'
image: '/home.webp'
tags:
  - 人工智能
badge: 人工智能
---


> 本文为原文的翻译，本文原链接转载自https://www.basedlabs.ai/articles/the-dangers-of-ai

这篇文章讨论了人工智能可能对人类构成的巨大风险，无论它是否有意识。以下是对文章的中文翻译：

## 人工智能可能极其危险 -> 无论它是否有意识

人工智能正在以惊人的速度进步，我们可能很快就会失去对它的控制，给人类带来巨大的风险。

想象一下：我们不会期望一个新生儿能击败国际象棋大师。那么，我们为什么会期望能够控制一个超级智能的人工智能呢？我们不能简单地关掉它——它已经想到了我们可能会尝试的所有方法，并阻止了我们。

或者考虑一下：一个超级智能的人工智能可以在一秒钟内完成100名人类工程师需要一年才能完成的工作。它大约可以在一秒钟内设计出一架新飞机或武器系统。

“我曾经认为人工智能变得比人类更聪明还很远。不再了，” Geoffrey Hinton 说，他是一位顶级的人工智能科学家，最近离开了谷歌，以警告人工智能的危险。

他并不孤单。2023年的一项调查发现，36%的人工智能专家担心人工智能可能造成“核级别灾难”。包括像史蒂夫·沃兹尼亚克和埃隆·马斯克这样的科技领袖在内的近28,000人签署了一封信，要求暂停高级人工智能开发六个月。

作为一名意识研究者，我也分享这些担忧，并签署了这封信。

有人说这些大型语言模型只是没有意识的花哨机器，所以它们不太可能逃脱。我同意它们可能还没有意识，但这并不重要。一枚核弹可以在没有意识的情况下杀死数百万人。人工智能也可以做到同样的事情，无论是直接（不太可能）还是通过操纵人类（更有可能）。

因此，关于人工智能意识的辩论对于人工智能安全来说并不重要。即使是看似无害的应用，如人工智能男友，如果使用不当，也可能带来风险。

我们为什么这么担心？简单来说：人工智能发展得太快了。

主要问题是新的聊天机器人或“大型语言模型”（LLMs）在对话方面的进步速度有多快。这种快速增长可能很快就会导致“人工通用智能”（AGI），在这种情况下，人工智能可以在没有人类的情况下自我改进。当这种情况发生时，我们可能无法控制它。

这并不是夸张。我们可能只有一次尝试的机会，如果我们搞砸了，我们可能没有机会再试一次。

微软的研究人员测试了OpenAI的GPT-4，他们说它显示出“高级通用智能的火花”。它在律师资格考试上的得分超过了90%的人类，而之前的版本只有10%。他们在许多其他测试中也看到了类似的进步。

这些主要是推理测试。这就是为什么研究人员认为GPT-4可能是AGI的早期版本。

这种快速的变化是为什么Hinton告诉《纽约时报》：“看看五年前的情况，再看看现在。把差异向前推，这是可怕的。” OpenAI的Sam Altman甚至告诉参议院，监管人工智能是“至关重要的”。

当我们把这些人工智能放入机器人时，它们将以同样的超级智能在现实世界中行动，并且能够以惊人的速度复制和改进自己。

我们尝试构建的任何安全措施一旦人工智能变得超级智能，都会被它轻易地识破和禁用。我们将无法控制它们，因为它们会以比我们快得多的速度想到我们可能做的所有事情。它们将突破我们设定的任何限制，就像格列佛逃脱了小人国的细小绳索一样。

一旦人工智能能够自我改进，这可能只是几年后甚至现在，我们不会知道它将做什么或如何控制它。一个超级智能的人工智能可以轻易地智胜人类，操纵我们，并在虚拟和物理世界中行动。

这被称为“控制问题”或“对齐问题”。像Nick Bostrom、Seth Baum和Eliezer Yudkowsky这样的专家已经研究了几十年。

是的，像GPT-4这样的模型已经存在。但人们要求的暂停是停止制造新的、更强大的模型。如果需要，我们可以通过关闭这些模型所需的庞大服务器场来执行这一点。

我认为，创建我们已知在不久的将来无法控制的系统是非常不明智的。我们需要知道何时从边缘退后。现在就是那个时候。

我们不应该比我们已经打开的潘多拉盒子更多地打开它。风险不仅限于基于文本的人工智能，影响从人工智能专辑封面生成工具到更具争议性的领域，如nsfw角色人工智能的各种应用。像泰勒·斯威夫特人工智能深度伪造这样的最近事件引发了对更严格政策的呼声，突显了在人工智能发展中需要谨慎考虑的紧迫性。


> 一下也给出了原文的链接

AI is improving so fast that we might soon lose control of it, posing huge risks to humanity


Think of it this way: We wouldn't expect a newborn to beat a chess grandmaster. So why would we expect to control a superintelligent AI? We can't just turn it off - it will have thought of every way we might try and stopped us.


Or consider this: a superintelligent AI could do in one second what 100 human engineers would take a year to do. It could design a new plane or weapon system in about a second.


"I used to think AI getting smarter than people was far off. Not anymore," says Geoffrey Hinton, a top AI scientist who recently quit Google to warn about AI dangers.


He's not alone. A 2023 survey found 36% of AI experts worry AI could cause a "nuclear-level catastrophe." Nearly 28,000 people, including tech leaders like Steve Wozniak and Elon Musk, have signed a letter asking for a six-month pause on advanced AI development.


As a consciousness researcher, I share these concerns and signed the letter too.


Some people say these LLMs are just fancy machines with no consciousness, so they're less likely to break free. I agree they probably aren't conscious yet, but that doesn't matter. A nuclear bomb can kill millions without being conscious. AI could do the same, either directly (less likely) or by manipulating humans (more likely).


So, debates about AI consciousness don't really matter for AI safety. Even seemingly benign applications like an ai boyfriend could potentially pose risks if not used properly.


Why are we so worried? Simply put: AI is developing too fast.


The main issue is how quickly new chatbots, or "large language models" (LLMs), are getting better at talking. This rapid growth could soon lead to "artificial general intelligence" (AGI), where AI can improve itself without humans. When that happens, we might not be able to control it.


This isn't exaggeration. We'll probably only get one shot at this, and if we mess up, we might not survive to try again.


Microsoft researchers testing OpenAI's GPT-4 say it shows "sparks of advanced general intelligence." It did better than 90% of humans on the bar exam for lawyers, up from just 10% for the previous version. They saw similar improvements on many other tests.


These are mostly reasoning tests. That's why the researchers think GPT-4 might be an early version of AGI.


This rapid change is why Hinton told the New York Times: "Look at how it was five years ago and how it is now. Take the difference and propagate it forwards. That's scary." OpenAI's Sam Altman even told the Senate that regulating AI is "crucial."


When we put these AIs in robots, they'll act in the real world with the same super-smarts, and they'll be able to copy and improve themselves incredibly fast.


Any safeguards we try to build will be easily figured out and disabled by the AI once it's superintelligent. We won't be able to control them because they'll think of everything we might do, way faster than us. They'll break free of any limits we set, like Gulliver escaping the tiny ropes of the Lilliputians.


Once AI can improve itself, which could be just a few years away or even now, we won't know what it'll do or how to control it. A superintelligent AI could easily outsmart humans, manipulate us, and act in both the virtual and physical world.


This is called the "control problem" or "alignment problem." Experts like Nick Bostrom, Seth Baum, and Eliezer Yudkowsky have studied it for decades.


Yes, models like GPT-4 are already out there. But the pause people are asking for is to stop making new, more powerful models. We can enforce this if needed by shutting down the massive server farms these models need.


I think it's very unwise to create systems we already know we won't be able to control in the near future. We need to know when to step back from the edge. Now is that time.


We shouldn't open Pandora's box any more than we already have. The risks extend beyond just text-based AI, affecting various applications from ai album cover generator tools to more controversial areas like nsfw character ai. Recent incidents, such as taylor swift ai deepfakes spark call for tougher policies, highlight the urgent need for careful consideration in AI development.